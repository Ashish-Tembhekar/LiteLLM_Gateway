# LiteLLM Microservices Architecture

A production-ready microservices setup with separate API service and WebUI client.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Client Browser                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ HTTP
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WebUI (Port 5500)                         â”‚
â”‚  - Flask web server                                          â”‚
â”‚  - Serves HTML/CSS/JS                                        â”‚
â”‚  - Proxies requests to API service                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ HTTP/REST API
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                LiteLLM API Service (Port 5000)               â”‚
â”‚  - Flask REST API                                            â”‚
â”‚  - Handles LLM requests via LiteLLM                          â”‚
â”‚  - SQLite database for tracking                              â”‚
â”‚  - Token counting & cost calculation                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ API Calls
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LLM Providers (OpenAI, Anthropic, etc.)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
litellm-crash-course/
â”œâ”€â”€ litellm_service/          # API Service (Port 5000)
â”‚   â”œâ”€â”€ api_service.py        # Main Flask API
â”‚   â”œâ”€â”€ requirements.txt      # Python dependencies
â”‚   â””â”€â”€ litellm_usage.db      # SQLite database (auto-created)
â”‚
â”œâ”€â”€ webui/                    # WebUI Client (Port 5500)
â”‚   â”œâ”€â”€ app.py                # Flask web server
â”‚   â”œâ”€â”€ requirements.txt      # Python dependencies
â”‚   â””â”€â”€ templates/
â”‚       â”œâ”€â”€ index.html        # Chat interface
â”‚       â””â”€â”€ usage.html        # Usage statistics page
â”‚
â””â”€â”€ .env                      # API keys (shared)
```

## ğŸš€ Quick Start

### Step 1: Install Dependencies

**For API Service:**
```bash
cd litellm_service
pip install -r requirements.txt
```

**For WebUI:**
```bash
cd webui
pip install -r requirements.txt
```

Or install both at once from the root:
```bash
pip install -r litellm_service/requirements.txt
pip install -r webui/requirements.txt
```

### Step 2: Configure API Keys

Make sure your `.env` file in the root directory has the required API keys:
```bash
OPENAI_API_KEY=sk-proj-xxxxx
ANTHROPIC_API_KEY=sk-ant-xxxxx
GOOGLE_API_KEY=AIzaSyxxxxx
GROQ_API_KEY=gsk_xxxxx
# ... add other providers as needed
```

### Step 3: Start the Services

**Terminal 1 - Start API Service:**
```bash
cd litellm_service
python api_service.py
```

**Terminal 2 - Start WebUI:**
```bash
cd webui
python app.py
```

### Step 4: Access the Application

- **WebUI**: http://localhost:5500
- **API Service**: http://localhost:5000
- **Usage Stats**: http://localhost:5500/usage

## ğŸ“¡ API Endpoints

### LiteLLM API Service (Port 5000)

#### 1. **POST /api/chat** - Make LLM Request
Request:
```json
{
  "application_id": "my-app",     // optional
  "user_id": "user123",           // optional
  "query": "What is AI?",         // required
  "model": "gpt-4o"               // required
}
```

Response:
```json
{
  "request_id": "uuid-here",
  "response": "AI is...",
  "metadata": {
    "response_time": 1.234,
    "tokens": {
      "prompt": 10,
      "completion": 50,
      "total": 60
    },
    "cost_usd": 0.00015,
    "timestamp": "2025-11-03T12:00:00",
    "model": "gpt-4o",
    "application_id": "my-app",
    "user_id": "user123"
  }
}
```

#### 2. **GET /api/user/{user_id}/usage** - Get User Statistics
Response:
```json
{
  "user_id": "user123",
  "summary": {
    "total_requests": 100,
    "total_tokens": 50000,
    "total_cost_usd": 0.5,
    "avg_response_time": 1.5,
    "apps_used": 2,
    "models_used": 3
  },
  "usage_by_application": [...],
  "usage_by_model": [...],
  "recent_requests": [...]
}
```

#### 3. **GET /api/models** - Get Available Models
Returns list of all available models grouped by provider.

#### 4. **GET /api/stats** - Get Global Statistics
Returns aggregate statistics across all users.

#### 5. **GET /health** - Health Check
Returns service health status.

## ğŸ’¾ Database Schema

The API service uses SQLite with the following schema:

```sql
CREATE TABLE requests (
    id TEXT PRIMARY KEY,              -- Unique request ID
    application_id TEXT,              -- Which app made the request
    user_id TEXT,                     -- Which user made the request
    model TEXT NOT NULL,              -- Model used
    query TEXT NOT NULL,              -- User's query
    response TEXT,                    -- LLM's response
    response_time REAL,               -- Time taken (seconds)
    prompt_tokens INTEGER,            -- Input tokens
    completion_tokens INTEGER,        -- Output tokens
    total_tokens INTEGER,             -- Total tokens
    cost_usd REAL,                    -- Estimated cost
    timestamp TEXT NOT NULL,          -- When request was made
    status TEXT,                      -- success/error
    error_message TEXT                -- Error details if failed
);
```

## ğŸ”§ Configuration

### Environment Variables

**API Service:**
- All LLM provider API keys (see `.env.example`)

**WebUI:**
- `LITELLM_API_URL` - URL of API service (default: http://localhost:5000)

### Changing Ports

**API Service** - Edit `litellm_service/api_service.py`:
```python
app.run(debug=True, port=5000, host='0.0.0.0')  # Change 5000 to your port
```

**WebUI** - Edit `webui/app.py`:
```python
app.run(debug=True, port=5500, host='0.0.0.0')  # Change 5500 to your port
```

## ğŸ“Š Features

### API Service Features
- âœ… Request/Response tracking in SQLite
- âœ… Token counting and cost calculation
- âœ… User-based usage statistics
- âœ… Application-based tracking
- âœ… Model performance metrics
- âœ… Error logging and handling
- âœ… CORS enabled for WebUI access
- âœ… RESTful API design

### WebUI Features
- âœ… Beautiful, modern chat interface
- âœ… Real-time API connection status
- âœ… User ID customization
- âœ… Model selection from all providers
- âœ… Response time display
- âœ… Token usage display
- âœ… Cost tracking per message
- âœ… Request ID tracking
- âœ… Comprehensive usage statistics page
- âœ… Usage breakdown by app and model
- âœ… Recent requests history

## ğŸ§ª Testing the API

### Using cURL

**Make a chat request:**
```bash
curl -X POST http://localhost:5000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "test-user",
    "application_id": "curl-test",
    "query": "Hello, how are you?",
    "model": "gpt-3.5-turbo"
  }'
```

**Get user statistics:**
```bash
curl http://localhost:5000/api/user/test-user/usage
```

**Get available models:**
```bash
curl http://localhost:5000/api/models
```

### Using Python

```python
import requests

# Make a chat request
response = requests.post('http://localhost:5000/api/chat', json={
    'user_id': 'python-user',
    'application_id': 'python-script',
    'query': 'What is machine learning?',
    'model': 'gpt-4o'
})

data = response.json()
print(f"Response: {data['response']}")
print(f"Cost: ${data['metadata']['cost_usd']}")
print(f"Tokens: {data['metadata']['tokens']['total']}")

# Get user stats
stats = requests.get('http://localhost:5000/api/user/python-user/usage').json()
print(f"Total requests: {stats['summary']['total_requests']}")
print(f"Total cost: ${stats['summary']['total_cost_usd']}")
```

## ğŸ”’ Security Considerations

### For Production Deployment:

1. **API Authentication**: Add API key authentication to the API service
2. **Rate Limiting**: Implement rate limiting to prevent abuse
3. **HTTPS**: Use HTTPS for both services
4. **Database**: Consider PostgreSQL for production instead of SQLite
5. **Environment Variables**: Use proper secret management
6. **CORS**: Restrict CORS to specific origins
7. **Input Validation**: Add comprehensive input validation
8. **Logging**: Implement proper logging and monitoring

## ğŸ“ˆ Monitoring & Analytics

The system tracks:
- Total requests per user
- Token usage per user/app/model
- Cost per user/app/model
- Average response times
- Error rates
- Model usage patterns

Access statistics at: http://localhost:5500/usage

## ğŸ› Troubleshooting

### API Service won't start
- Check if port 5000 is already in use
- Verify API keys are set in `.env`
- Check Python dependencies are installed

### WebUI can't connect to API
- Ensure API service is running on port 5000
- Check CORS is enabled in API service
- Verify `LITELLM_API_URL` is correct

### Database errors
- Delete `litellm_usage.db` to reset database
- Check file permissions
- Ensure SQLite is available

### LLM requests failing
- Verify API keys are valid
- Check provider service status
- Review error messages in API response

## ğŸš¢ Deployment

### Docker Deployment (Recommended)

Create `docker-compose.yml`:
```yaml
version: '3.8'
services:
  api:
    build: ./litellm_service
    ports:
      - "5000:5000"
    env_file:
      - .env
    volumes:
      - ./data:/app/data
  
  webui:
    build: ./webui
    ports:
      - "5500:5500"
    environment:
      - LITELLM_API_URL=http://api:5000
    depends_on:
      - api
```

### Cloud Deployment
- **API Service**: Deploy to AWS Lambda, Google Cloud Run, or Azure Functions
- **WebUI**: Deploy to Vercel, Netlify, or any static hosting
- **Database**: Use managed PostgreSQL (AWS RDS, Google Cloud SQL)

## ğŸ“ License

MIT

## ğŸ¤ Contributing

Contributions welcome! Please open an issue or PR.

---

**Built with â¤ï¸ using LiteLLM, Flask, and SQLite**

